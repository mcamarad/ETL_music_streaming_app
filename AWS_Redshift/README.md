# Summary

An Startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

I created a SQL Database Schema for AWS Redshift to enable Sparkify to analyze their data. The given Schema can be filled with data using the provided ETL pipeline.

# How to run the python scripts

To create the database tables and run the ETL pipeline, you must run the following two files in the order that they are listed below

To create tables:
```bash
python create_tables.py
```
To populate tables using the ETL pipeline:
```bash
python etl.py
```

# Files in the repository


* **[create_tables.py](create_tables.py)**: Python script to perform postgreSQL commands for creation/reset of the database and tables.
* **[sql_queries.py](sql_queries.py)**: Python script containing postgreSQL commands used by create_tables.py and etl.py.
* **[etl.py](etl.py)**: Python script to extract the information from Song and Log data within the data folder and inserting them to the database and tables.

# Objective

Storing data in a database makes it easier for ad-hoc analysis. Using postgreSQL and the START scheme, joins and aggregations allows Analysts to query and access data in a comprehensive manner. By using relational databases, Sparkify can access and extract valuable information from it user logs.

# The database schema design and ETL pipeline.

Here, a Relational Database Schema was created to enable Sparkify to analyze their data. This database was then populated with an ETL pipeline.

The star scheme enables the company to view the user behaviour over several dimensions. The fact table is used to store all user song activities that contain the category "NextSong". Using this table, the company can relate and analyze the different dimensions of this schema: users, songs, artists and time.

In order to populate the relational database, an ETL pipeline was used. The ETL pipeline made possible to extract the relevant information from the log files of the user behaviour as well as the corresponding data from the songs and load it into the schema. I used the automatic distribution of Redshift.

* **Fact Table**: songplays
* **Dimension Tables**: users, songs, artists and time.

# Datasets

The data is extracted from S3 buckets hosted at AWS

* **Song data**: ```s3://udacity-dend/song_data```
* **Log data**: ```s3://udacity-dend/log_data```

The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.